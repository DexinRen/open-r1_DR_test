This project is a customized fork of [huggingface/openr1](https://github.com/huggingface/open-r1). Due to GitHubâ€™s upload limitations, datasets, model checkpoints, and log files are excluded from the repository. This guide explains how to reconstruct them and reproduce the results in the project report.

----

#### Dataset Recovery: 
We use Math220k as the training dataset and for testing purposes. Since there is no testing split in Math220k, we need to create our own.  Also, our model evaluation process with bootstrap=True requires a local version of the bootstrapped (to 1000 data volumes by default) datasets (math220k-train, math220k-test, math500, gpqa) to make sure every checkpoints are evaluated under the same datasets with the same data volume (this is very helpful on gpqa and small dataset for decreasing the randomness caused by small dataset size). To recover the local dataset (including the bootstrapped version), **run dexin_src/utils/dataset_customizer.py**

----
#### SFT Process:
In this project, we use sft_auto.py as our training script (use **bash dexin_src/run_sft.sh** to run it), the config file for the sft_auto.py is dexin_src/config/run_config.yaml (where we save model arguments, training arguments, and Trainer config). Since this is fine-tuning, we use a small learning rate of 2.5e-6 (with learning rate scheduler type as cosine). The checkpoint will be saved every 500 steps under the data folder.

P.S. The env include lighteval, evaluate, Trainer API used in open-r1 project (based on the env install guide on official open-r1 README file) are fixed, restricted, and old, its nearly impossible to let the trainer/evaluator/lighteval to evaluate the eval_dataset so we have to use eval_checkpoints_auto.py to evaluate all checkpoints after training, and determine which checkpoint is the desired one (and also **DO NOT USE** the official env, or everything will be broken, for example, Trainer.log_metrics and Trainer.evaluate() is not functioning at all because evaluation during training was broken under the Open-R1 env.).

----

#### Model Evaluation:
Since the whole open-r1 env is broken (we cannot use lighteval, trainer.evaluate(), log_matrics, or anything alike), we have to use eval_checkpoints_auto.py to automatically evaluate every checkpoint over math220k-train, math220k-test, math500, gpqa-diamond, and gpqa-extended. The script will also record all prompts and generations that were graded as wrong, and also logs of unparsable (answer un-extractable) generated text (under dexin_src/eval_output/generations) for further inspection. This script supports the **bootstrap method** (bootstrapped dataset generated by dexin_src/utils/dataset_customizer.py function bootstrap_to_size() ) to balance each dataset's size to 1000. If any other size is desired, use dexin_src/utils/dataset_customizer.py with a customized size to generate the dataset and manually change the dataset path in eval_checkpoints_auto.py to do the evaluation. If bootstrap is unwanted, set bootstrap=False to fallback to vanilla evaluation.

We applied bootstrap sampling (size=1000) for evaluation across all datasets (math220k-train, math220k-test, math500, and GPQA). This approach provides several practical and methodological advantages:
- Stability: Evaluating on a fixed-size bootstrapped set reduces the variance introduced by small datasets (e.g., GPQA) and ensures consistent sample size across tasks.
- Efficiency: 1,000 samples offer a balance between statistical reliability and evaluation speed, making it feasible to test many checkpoints locally.
- Comparability: All checkpoints are evaluated on the same fixed bootstrapped datasets, ensuring that improvements reflect model changes, not dataset fluctuations.

The evaluation score on training on-hold split is largely different than the training loss/acc on the checkpoints file, since stf needs to evaluate the generation with the dataset's solution and answer, and our result only compares the answer. 

We only evaluate on the answer because the Math220k dataset's solution is not filtered (**the whole Math220k was AI-generated and noisy**). And it is very common to extract different answers from the solution and the answer in the dataset. Also, the format of the answer is not fixed. For example, in single-choice questions, the answer column in Math220k can be an option letter, the value of an option, or even an option letter + value. Moreover, some questions' answers should include symbols and/or units, but some answers did not, and they could include strange symbols, such as one dollar sign for no reason. 

In conclusion, the whole dataset is extremely noisy. Openr1 can only affirm that the dataset's default split is filtered based on the answer, so the solution can only be used as a second-prioritized answer extraction source. **Evaluation on the solution is not recommended** (it's also not recommended to train with this dataset's solution, but this is a supervised learning, and this dataset is the default dataset of the open-r1 project, so we have to use this)

For GPQA evaluation, we applied strict and soft answer extraction. Strict answer extraction will only extract answer X in "Answer: X" or "\boxed{X}", as required in the customized prompt (and this is also a standard way to mark the answer); The soft extraction will try to extract answer with informal marks such as "so the correct option is", "the answer to this question is", and et, cl. But it's impossible to iterate over every ill-formatted answer, so there are still some ill-formatted answers that stay unparsable and get marked as wrong. In the result, all GPQA results with _clean suffix represent the accuracy calculated on the answer extracted with strict answer extraction, and _all suffix is the accuracy calculated based on all answers extracted (strict + soft).

The evaluation result will be stored under dexin_src/eval_output as eval_summary.csv or eval_summary_bootstrap.csv

To do the evaluation, **run dexin_src/eval_checkpoints_auto.py**, change bootstrap=False to return to vanilla evaluation.


