This is a version of open-r1 project forked from [huggingface/openr1](https://github.com/huggingface/open-r1)
Since there is an upload limit in GitHub, we have to commit without the dataset, checkpoints, and log file. So here is a way to recover them and use the project script to re-generate the result on the project report.

#### Dataset Recovery: 
We use Math220k as the training dataset and for testing purposes. Since there is no testing split in Math220k, we need to create our own.  Also, our model evaluation process with bootstrap=True requires a local version of the bootstrapped (to 1000 data volumes by default) datasets (math220k-train, math220k-test, math500, gpqa) to make sure every checkpoints are evaluated under the same datasets with the same data volume (this is very helpful on gpqa and small dataset for decreasing the randomness caused by small dataset size). To recover the local dataset (including the bootstrapped version), **run dexin_src/utils/dataset_customizer.py**

#### SFT Process:
In this project, we use sft_auto.py as our training script (use **bash dexin_src/run_sft.sh** to run it), the config file for the sft_auto.py is dexin_src/config/run_config.yaml (where we save model arguments, training arguments, and Trainer config). Since this is fine-tuning, we use a small learning rate of 2.5e-6 (with learning rate scheduler type as cosine). The checkpoint will be saved every 500 steps under the data folder.

P.S. The env include lighteval, evaluate, Trainer API used in open-r1 project (based on the env install guide on official open-r1 README file) are fixed, restricted, and old, its nearly impossible to let the trainer/evaluator/lighteval to evaluate the eval_dataset so we have to use eval_checkpoints_auto.py to evaluate all checkpoints after training, and determine which checkpoint is the desired one (and also **DO NOT USE** the official env, or everything will be broken).

#### Model Evaluation:
We use eval_checkpoints_auto.py to automatically evaluate every checkpoint over math220k-train, math220k-test, math500, gpqa-diamond, and gpqa-extended. The script will also record all prompts and generations that were graded as wrong, and also logs of unparsable (answer un-extractable) generated text (under dexin_src/eval_output/generations) for further inspection. This script supports the bootstrap method to balance each dataset's size to 1000. If any other size is desired, use dexin_src/utils/dataset_customizer.py with a customized size to generate the dataset and manually change the dataset path in eval_checkpoints_auto.py to do the evaluation. If bootstrap is unwanted, set bootstrap=False to fallback to vanilla evaluation.

The evaluation score on training on-hold split is largely different than the training loss/acc on the checkpoints file, since stf needs to evaluate the generation with the dataset's solution and answer, and our result only compares the answer. The reason we do this is that the Math220k dataset's solution is never filtered (the whole dataset was AI-generated). And it is very common to extract different answers from the solution and the answer in the dataset. Also, the format of the answer is not fixed (for example, in single-choice questions, the answer column in Math220k can be an option letter, the value of an option, and even an option letter + value). 

Also, some questions' answers should include symbols and/or units (but some answers did not), and they could include strange symbols, such as one dollar sign for no reason. In conclusion, the whole dataset is extremely noisy, it's not recommended to grade the model with the solution since it's not promised to represent a correct reasoning process. Openr1 can only affirm that the dataset's default split is filtered based on the answer, so the solution can only be used as second-prioritized answer extraction source.

For GPQA evaluation, we applied strict and soft answer extraction. Strict answer extraction will only extract answer X in "Answer: X" or "\boxed{X}", as required in the customized prompt (and this is also a standard way to mark the answer); The soft extraction will try to extract answer with informal marks such as "so the correct option is", "the answer to this question is", and et, cl. But it's impossible to iterate over every ill-formatted answer, so there are still some ill-formatted answers that stay unparsable and get marked as wrong. In the result, all GPQA results with _clean suffix represent the accuracy calculated on the answer extracted with strict answer extraction, and _all suffix is the accuracy calculated based on all answers extracted (strict + soft).

The evaluation result will be stored under dexin_src/eval_output as eval_summary.csv or eval_summary_bootstrap.csv


